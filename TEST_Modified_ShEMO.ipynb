{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyzd95/ShEMO-Modification/blob/main/TEST_Modified_ShEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUVf6aU53Nsn",
        "outputId": "44c0f743-9d2d-405e-f75d-de7fe61384ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  982M  100  982M    0     0   140M      0  0:00:07  0:00:07 --:--:--  151M\n"
          ]
        }
      ],
      "source": [
        "!curl --remote-name-all https://storage.googleapis.com/danielk-files/farsi-text/merged_files/w2c_merged.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6l2noUmI_yH",
        "outputId": "7ca9f253-d23c-4e40-971c-d3183f3a94f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting parsivar\n",
            "  Downloading parsivar-0.2.3.tar.gz (36.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.2 MB 426 kB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->parsivar) (1.15.0)\n",
            "Building wheels for collected packages: parsivar, nltk\n",
            "  Building wheel for parsivar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsivar: filename=parsivar-0.2.3-py3-none-any.whl size=36492971 sha256=3af7273a7f842ab393b62dd8b4a848040367e58b151c3914f569681c4a3f118a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/67/7a/49cbf08f64d3f76a26eceaf0e481a40e233f05d4356875cbed\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449922 sha256=e94852fa136ad18a18b6c37840255d1ce1add10e58eeb9bd65301f8336c8fb10\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "Successfully built parsivar nltk\n",
            "Installing collected packages: nltk, parsivar\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed nltk-3.4.5 parsivar-0.2.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2fawords\n",
            "  Downloading num2fawords-1.1-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: num2fawords\n",
            "Successfully installed num2fawords-1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 51.3 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=8f5159d813de1abd2a8f6e6d615d6a429b5a8ef2e83897c52b8054876144612b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154800 sha256=16e99c150e1a594862936d2411478b06fea4330bd71f6a30fee05fd3c5cc143f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.4.5\n",
            "    Uninstalling nltk-3.4.5:\n",
            "      Successfully uninstalled nltk-3.4.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "parsivar 0.2.3 requires nltk==3.4.5, but you have nltk 3.3 which is incompatible.\u001b[0m\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install parsivar\n",
        "!pip install num2fawords\n",
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7wBPu4Hw2Wk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "from hazm import *\n",
        "from num2fawords import words\n",
        "\n",
        "sentence_tokenizer = SentenceTokenizer()\n",
        "_normalizer = Normalizer()\n",
        "\n",
        "chars_to_ignore = [\n",
        "    \",\", \"?\", \".\", \"!\", \"-\", \";\", \":\", '\"\"', \"%\", \"'\", '\"', \"�\",\n",
        "    \"#\", \"!\", \"؟\", \"?\", \"«\", \"»\", \"،\", \"(\", \")\", \"؛\", \"'ٔ\", \"٬\", 'ٔ', \",\", \"?\",\n",
        "    \".\", \"!\", \"-\", \";\", \":\", '\"', \"“\", \"%\", \"‘\", \"”\", \"�\", \"–\", \"…\", \"_\", \"”\", '“', '„',\n",
        "    'ā', 'š',\n",
        "    # \"ء\",\n",
        "]\n",
        "\n",
        "# In case of farsi\n",
        "chars_to_ignore = chars_to_ignore + list(string.ascii_lowercase + string.digits)\n",
        "\n",
        "chars_to_mapping = {\n",
        "    'ك': 'ک', 'دِ': 'د', 'بِ': 'ب', 'زِ': 'ز', 'ذِ': 'ذ', 'شِ': 'ش', 'سِ': 'س', 'ى': 'ی',\n",
        "    'ي': 'ی', 'أ': 'ا', 'ؤ': 'و', \"ے\": \"ی\", \"ۀ\": \"ه\", \"ﭘ\": \"پ\", \"ﮐ\": \"ک\", \"ﯽ\": \"ی\",\n",
        "    \"ﺎ\": \"ا\", \"ﺑ\": \"ب\", \"ﺘ\": \"ت\", \"ﺧ\": \"خ\", \"ﺩ\": \"د\", \"ﺱ\": \"س\", \"ﻀ\": \"ض\", \"ﻌ\": \"ع\",\n",
        "    \"ﻟ\": \"ل\", \"ﻡ\": \"م\", \"ﻢ\": \"م\", \"ﻪ\": \"ه\", \"ﻮ\": \"و\", 'ﺍ': \"ا\", 'ة': \"ه\",\n",
        "    'ﯾ': \"ی\", 'ﯿ': \"ی\", 'ﺒ': \"ب\", 'ﺖ': \"ت\", 'ﺪ': \"د\", 'ﺮ': \"ر\", 'ﺴ': \"س\", 'ﺷ': \"ش\",\n",
        "    'ﺸ': \"ش\", 'ﻋ': \"ع\", 'ﻤ': \"م\", 'ﻥ': \"ن\", 'ﻧ': \"ن\", 'ﻭ': \"و\", 'ﺭ': \"ر\", \"ﮔ\": \"گ\",\n",
        "\n",
        "    # \"ها\": \"  ها\", \"ئ\": \"ی\",\n",
        "\n",
        "    \"a\": \" ای \", \"b\": \" بی \", \"c\": \" سی \", \"d\": \" دی \", \"e\": \" ایی \", \"f\": \" اف \",\n",
        "    \"g\": \" جی \", \"h\": \" اچ \", \"i\": \" آی \", \"j\": \" جی \", \"k\": \" کی \", \"l\": \" ال \",\n",
        "    \"m\": \" ام \", \"n\": \" ان \", \"o\": \" او \", \"p\": \" پی \", \"q\": \" کیو \", \"r\": \" آر \",\n",
        "    \"s\": \" اس \", \"t\": \" تی \", \"u\": \" یو \", \"v\": \" وی \", \"w\": \" دبلیو \", \"x\": \" اکس \",\n",
        "    \"y\": \" وای \", \"z\": \" زد \",\n",
        "    \"\\u200c\": \" \", \"\\u200d\": \" \", \"\\u200e\": \" \", \"\\u200f\": \" \", \"\\ufeff\": \" \",\n",
        "}\n",
        "\n",
        "\n",
        "def multiple_replace(text, chars_to_mapping):\n",
        "    pattern = \"|\".join(map(re.escape, chars_to_mapping.keys()))\n",
        "    return re.sub(pattern, lambda m: chars_to_mapping[m.group()], str(text))\n",
        "\n",
        "\n",
        "def remove_special_characters(text, chars_to_ignore_regex):\n",
        "    text = re.sub(chars_to_ignore_regex, '', text).lower() + \" \"\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalizer(text, chars_to_ignore=chars_to_ignore, chars_to_mapping=chars_to_mapping):\n",
        "    chars_to_ignore_regex = f\"\"\"[{\"\".join(chars_to_ignore)}]\"\"\"\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    text = _normalizer.normalize(text)\n",
        "    text = multiple_replace(text, chars_to_mapping)\n",
        "    text = remove_special_characters(text, chars_to_ignore_regex)\n",
        "    text = re.sub(\" +\", \" \", text)\n",
        "    _text = []\n",
        "    for word in text.split():\n",
        "        try:\n",
        "            word = int(word)\n",
        "            _text.append(words(word))\n",
        "        except:\n",
        "            _text.append(str(word))\n",
        "\n",
        "    text = \" \".join(_text) + \" \"\n",
        "\n",
        "    text = text.strip() + \" \"\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sentences():\n",
        "    sentences = []\n",
        "    with open('w2c_merged.txt', 'r', encoding='utf-8' ) as f:\n",
        "      for line in f:\n",
        "        lines = sentence_tokenizer.tokenize(line)\n",
        "        for l in lines:\n",
        "            ll = normalizer(l)\n",
        "            sentences.append(ll)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "sents = load_sentences()\n",
        "with open('w2c_cleaned.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write('\\n'.join(sents))"
      ],
      "metadata": {
        "id": "vYlHavdSoC_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58OKy4bWxg-Z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
        "!git clone https://github.com/kpu/kenlm\n",
        "%cd kenlm\n",
        "!mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make -j 4\n",
        "!make install\n",
        "%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwUpTwyIxbqQ",
        "outputId": "5da752c8-5f8e-41a5-efb9-c10c16c53b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/w2c_cleaned.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2173591552 bytes == 0x557933134000 @  0x7f5dea8021e7 0x5579308ea912 0x55793088562e 0x55793086441b 0x557930850176 0x7f5de899bc87 0x557930851cda\n",
            "tcmalloc: large alloc 8694341632 bytes == 0x5579b4a1a000 @  0x7f5dea8021e7 0x5579308ea912 0x5579308d993a 0x5579308da378 0x557930864438 0x557930850176 0x7f5de899bc87 0x557930851cda\n",
            "****************************************************************************************************\n",
            "Unigram tokens 129533510 types 796985\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:9563820 2:1852514944 3:3473465600 4:5557544960\n",
            "tcmalloc: large alloc 5557551104 bytes == 0x557933134000 @  0x7f5dea8021e7 0x5579308ea912 0x5579308d993a 0x5579308da378 0x557930864a07 0x557930850176 0x7f5de899bc87 0x557930851cda\n",
            "tcmalloc: large alloc 1852522496 bytes == 0x557a7ee74000 @  0x7f5dea8021e7 0x5579308ea912 0x5579308d993a 0x5579308da378 0x557930864e0d 0x557930850176 0x7f5de899bc87 0x557930851cda\n",
            "tcmalloc: large alloc 3473473536 bytes == 0x557bbb64c000 @  0x7f5dea8021e7 0x5579308ea912 0x5579308d993a 0x5579308da378 0x557930864e0d 0x557930850176 0x7f5de899bc87 0x557930851cda\n",
            "Statistics:\n",
            "1 796985 D1=0.725184 D2=1.02439 D3+=1.28672\n",
            "2 14500785 D1=0.752964 D2=1.08474 D3+=1.36026\n",
            "3 47526993 D1=0.845194 D2=1.15487 D3+=1.35154\n",
            "4 73098618 D1=0.750173 D2=1.38691 D3+=1.74152\n",
            "Memory estimate for binary LM:\n",
            "type      MB\n",
            "probing 2694 assuming -p 1.5\n",
            "probing 3052 assuming -r models -p 1.5\n",
            "trie    1274 without quantization\n",
            "trie     726 assuming -q 8 -b 8 quantization \n",
            "trie    1134 assuming -a 22 array pointer compression\n",
            "trie     586 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:9563820 2:232012560 3:950539860 4:1754366832\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:9563820 2:232012560 3:950539860 4:1754366832\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:14373500 kB\tVmRSS:2904736 kB\tRSSMax:5051380 kB\tuser:152.748\tsys:26.4951\tCPU:179.244\treal:180.638\n"
          ]
        }
      ],
      "source": [
        "!/content/kenlm/build/bin/lmplz --text 'w2c_cleaned.txt' --arpa words.arpa --order 4 --discount_fallback --temp_prefix /tmp/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgv9HH1q5tia"
      },
      "outputs": [],
      "source": [
        "with open(\"words.arpa\", \"r\") as read_file, open(\"words_correct.arpa\", \"w\") as write_file:\n",
        "  has_added_eos = False\n",
        "  for line in read_file:\n",
        "    if not has_added_eos and \"ngram 1=\" in line:\n",
        "      count=line.strip().split(\"=\")[-1]\n",
        "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
        "    elif not has_added_eos and \"<s>\" in line:\n",
        "      write_file.write(line)\n",
        "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
        "      has_added_eos = True\n",
        "    else:\n",
        "      write_file.write(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XhG2TUNJnSj",
        "outputId": "d91cd5db-1c8a-4a0e-e030-871c67623b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.20.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 14.2 MB/s \n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 68.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.20.8 rapidfuzz-2.13.2\n"
          ]
        }
      ],
      "source": [
        "!pip install Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A97Bi7enJpIu"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# ----------------------------------------------------------------------------\n",
        "# Copyright 2015-2016 Nervana Systems Inc.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ----------------------------------------------------------------------------\n",
        "# Modified to support pytorch Tensors\n",
        "\n",
        "import Levenshtein as Lev\n",
        "import torch\n",
        "from six.moves import xrange\n",
        "\n",
        "\n",
        "class Decoder(object):\n",
        "    \"\"\"\n",
        "    Basic decoder class from which all other decoders inherit. Implements several\n",
        "    helper functions. Subclasses should implement the decode() method.\n",
        "    Arguments:\n",
        "        labels (list): mapping from integers to characters.\n",
        "        blank_index (int, optional): index for the blank '_' character. Defaults to 0.\n",
        "        space_index (int, optional): index for the space ' ' character. Defaults to 28.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, labels, blank_index=0):\n",
        "        self.labels = labels\n",
        "        self.int_to_char = dict([(i, c) for (i, c) in enumerate(labels)])\n",
        "        self.blank_index = blank_index\n",
        "        space_index = len(labels)  # To prevent errors in decode, we add an out of bounds index for the space\n",
        "        if ' ' in labels:\n",
        "            space_index = labels.index(' ')\n",
        "        self.space_index = space_index\n",
        "\n",
        "    def wer(self, s1, s2):\n",
        "        \"\"\"\n",
        "        Computes the Word Error Rate, defined as the edit distance between the\n",
        "        two provided sentences after tokenizing to words.\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "\n",
        "        # build mapping of words to integers\n",
        "        b = set(s1.split() + s2.split())\n",
        "        word2char = dict(zip(b, range(len(b))))\n",
        "\n",
        "        # map the words to a char array (Levenshtein packages only accepts\n",
        "        # strings)\n",
        "        w1 = [chr(word2char[w]) for w in s1.split()]\n",
        "        w2 = [chr(word2char[w]) for w in s2.split()]\n",
        "\n",
        "        return Lev.distance(''.join(w1), ''.join(w2))\n",
        "\n",
        "    def cer(self, s1, s2):\n",
        "        \"\"\"\n",
        "        Computes the Character Error Rate, defined as the edit distance.\n",
        "        Arguments:\n",
        "            s1 (string): space-separated sentence\n",
        "            s2 (string): space-separated sentence\n",
        "        \"\"\"\n",
        "        s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
        "        return Lev.distance(s1, s2)\n",
        "\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Given a matrix of character probabilities, returns the decoder's\n",
        "        best guess of the transcription\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities, where probs[c,t]\n",
        "                            is the probability of character c at time t\n",
        "            sizes(optional): Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            string: sequence of the model's best guess for the transcription\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BeamCTCDecoder(Decoder):\n",
        "    def __init__(self, labels, lm_path=None, alpha=0, beta=0, cutoff_top_n=40, cutoff_prob=1.0, beam_width=100,\n",
        "                 num_processes=4, blank_index=0):\n",
        "        super(BeamCTCDecoder, self).__init__(labels)\n",
        "        try:\n",
        "            from ctcdecode import CTCBeamDecoder\n",
        "        except ImportError:\n",
        "            raise ImportError(\"BeamCTCDecoder requires paddledecoder package.\")\n",
        "        labels = list(labels)  # Ensure labels are a list before passing to decoder\n",
        "        self._decoder = CTCBeamDecoder(labels, lm_path, alpha, beta, cutoff_top_n, cutoff_prob, beam_width,\n",
        "                                       num_processes, blank_index, log_probs_input=True)\n",
        "\n",
        "    def convert_to_strings(self, out, seq_len):\n",
        "        results = []\n",
        "        for b, batch in enumerate(out):\n",
        "            utterances = []\n",
        "            for p, utt in enumerate(batch):\n",
        "                size = seq_len[b][p]\n",
        "                if size > 0:\n",
        "                    transcript = ''.join(map(lambda x: self.int_to_char[x.item()], utt[0:size]))\n",
        "                else:\n",
        "                    transcript = ''\n",
        "                utterances.append(transcript)\n",
        "            results.append(utterances)\n",
        "        return results\n",
        "\n",
        "    def convert_tensor(self, offsets, sizes):\n",
        "        results = []\n",
        "        for b, batch in enumerate(offsets):\n",
        "            utterances = []\n",
        "            for p, utt in enumerate(batch):\n",
        "                size = sizes[b][p]\n",
        "                if sizes[b][p] > 0:\n",
        "                    utterances.append(utt[0:size])\n",
        "                else:\n",
        "                    utterances.append(torch.tensor([], dtype=torch.int))\n",
        "            results.append(utterances)\n",
        "        return results\n",
        "\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Decodes probability output using ctcdecode package.\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities, where probs[c,t]\n",
        "                            is the probability of character c at time t\n",
        "            sizes: Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            string: sequences of the model's best guess for the transcription\n",
        "        \"\"\"\n",
        "        probs = probs.cpu()\n",
        "        out, scores, offsets, seq_lens = self._decoder.decode(probs, sizes)\n",
        "\n",
        "        strings = self.convert_to_strings(out, seq_lens)\n",
        "        offsets = self.convert_tensor(offsets, seq_lens)\n",
        "        return strings, offsets\n",
        "\n",
        "\n",
        "class GreedyDecoder(Decoder):\n",
        "    def __init__(self, labels, blank_index=0):\n",
        "        super(GreedyDecoder, self).__init__(labels, blank_index)\n",
        "\n",
        "    def convert_to_strings(self, sequences, sizes=None, remove_repetitions=False, return_offsets=False):\n",
        "        \"\"\"Given a list of numeric sequences, returns the corresponding strings\"\"\"\n",
        "        strings = []\n",
        "        offsets = [] if return_offsets else None\n",
        "        for x in xrange(len(sequences)):\n",
        "            seq_len = sizes[x] if sizes is not None else len(sequences[x])\n",
        "            string, string_offsets = self.process_string(sequences[x], seq_len, remove_repetitions)\n",
        "            strings.append([string])  # We only return one path\n",
        "            if return_offsets:\n",
        "                offsets.append([string_offsets])\n",
        "        if return_offsets:\n",
        "            return strings, offsets\n",
        "        else:\n",
        "            return strings\n",
        "\n",
        "    def process_string(self, sequence, size, remove_repetitions=False):\n",
        "        string = ''\n",
        "        offsets = []\n",
        "        for i in range(size):\n",
        "            char = self.int_to_char[sequence[i].item()]\n",
        "            if char != self.int_to_char[self.blank_index]:\n",
        "                # if this char is a repetition and remove_repetitions=true, then skip\n",
        "                if remove_repetitions and i != 0 and char == self.int_to_char[sequence[i - 1].item()]:\n",
        "                    pass\n",
        "                elif char == self.labels[self.space_index]:\n",
        "                    string += ' '\n",
        "                    offsets.append(i)\n",
        "                else:\n",
        "                    string = string + char\n",
        "                    offsets.append(i)\n",
        "        return string, torch.tensor(offsets, dtype=torch.int)\n",
        "\n",
        "    def decode(self, probs, sizes=None):\n",
        "        \"\"\"\n",
        "        Returns the argmax decoding given the probability matrix. Removes\n",
        "        repeated elements in the sequence, as well as blanks.\n",
        "        Arguments:\n",
        "            probs: Tensor of character probabilities from the network. Expected shape of batch x seq_length x output_dim\n",
        "            sizes(optional): Size of each sequence in the mini-batch\n",
        "        Returns:\n",
        "            strings: sequences of the model's best guess for the transcription on inputs\n",
        "            offsets: time step per character predicted\n",
        "        \"\"\"\n",
        "        _, max_probs = torch.max(probs, 2)\n",
        "        strings, offsets = self.convert_to_strings(max_probs.view(max_probs.size(0), max_probs.size(1)), sizes,\n",
        "                                                   remove_repetitions=True, return_offsets=True)\n",
        "        return strings, offsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayT0UttxJRtc",
        "outputId": "a5e86a77-d4bc-493c-d3f1-f51aed691927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79809 sha256=6998fa6d632fa29296ece07e0d7f6650dc431742f0632ce25fbe4d6a9373b2a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.12.1.post1\n",
            "    Uninstalling folium-0.12.1.post1:\n",
            "      Successfully uninstalled folium-0.12.1.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.17.5 requires folium>=0.9.1, but you have folium 0.2.1 which is incompatible.\u001b[0m\n",
            "Successfully installed folium-0.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[K     | 550 kB 2.7 MB/s\n",
            "\u001b[?25hCollecting pyctcdecode\n",
            "  Downloading pyctcdecode-0.4.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting pygtrie<3.0,>=2.1\n",
            "  Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from pyctcdecode) (1.21.6)\n",
            "Collecting hypothesis<7,>=6.14\n",
            "  Downloading hypothesis-6.58.0-py3-none-any.whl (396 kB)\n",
            "\u001b[K     |████████████████████████████████| 396 kB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
            "Collecting exceptiongroup>=1.0.0\n",
            "  Downloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (22.1.0)\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp37-cp37m-linux_x86_64.whl size=2369503 sha256=7232eb472c16ebf3e6aa3aaef8d4f75e4ee8425211f32d4069502988601ad5e5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oui8q6j_/wheels/3d/aa/02/7b4a2eab5d7a2a9391bd9680dbad6270808a147bc3b7047e4e\n",
            "Successfully built kenlm\n",
            "Installing collected packages: exceptiongroup, pygtrie, hypothesis, pyctcdecode, kenlm\n",
            "Successfully installed exceptiongroup-1.0.4 hypothesis-6.58.0 kenlm-0.0.0 pyctcdecode-0.4.0 pygtrie-2.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-ikec63ak\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-ikec63ak\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 13.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 62.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.25.0.dev0) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.25.0.dev0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.25.0.dev0) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.25.0.dev0) (2.10)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.25.0.dev0-py3-none-any.whl size=5758972 sha256=687b7731758dfb69b421f0ec47c84b6b9c71bde06771a9bed08dae3dc5af5ce3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4f8lmask/wheels/90/a5/44/6bcd83827c8a60628c5ad602f429cd5076bcce5f2a90054947\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.25.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip pyctcdecode\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-OcD4QhLB_D"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutoProcessor, Wav2Vec2ProcessorWithLM, Wav2Vec2CTCTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282,
          "referenced_widgets": [
            "e24a736eb8f343868a54afe61df3e0a6",
            "4e769fc5f5d240948eb5cdfea2ff012e",
            "e8cef2decccd423588a4e58b029dc36b",
            "02718a7498f1486985f2440a79b3c36a",
            "26487f36efa34cf8be8e2af419a3e7f3",
            "ac85399a3bf14e929b29c732d749862a",
            "ef8ff882046a45b28eb43f4711682d8a",
            "80eadb7cd5fd465ebb91d923d765c515",
            "c370f8800ba448308ecd7c3dc486a6d6",
            "b75d0f11cb9542ca98e0d5fe47856f70",
            "bbddbd127ebc4b469f6f824cd91842a6",
            "a93dc4e86f84436eb8fb9fd56beec584",
            "26ca0e0e56f8475ebdea130b0d2874e0",
            "cf25a94dc42941eb8cc161c23f2c3251",
            "364781697da042be8b98fbcddd79a28f",
            "db448bc5ab4048598d1f792eaa9689c7",
            "72ad2a1c45084b20a0117acedd81306a",
            "05a9387eff57442085babf70c8a5db90",
            "741fc76bcd4a48518cc8c476c37dcf87",
            "73a6e1c7334b4de78e88bd59ff5b69ba",
            "7ef711b17004409b9239081bb21e8848",
            "38477e25e57647b28e77cfc5303708cb",
            "6c0f7896c26a466a886c11539af885d6",
            "2146cb5839f642ecae2aa158869a3eac",
            "e87dc11bef0943c1a7ca10852b56dfae",
            "a770b58cc68d4de2a4c0f8d91952dbac",
            "2778260c305c462690938cc70b2aa5c3",
            "cce3820a925e4301aebddb0c40dd65a2",
            "645911aaf5684ed5af9f5f45206f3b84",
            "4bc9f928a00441619ecf0271525d4d34",
            "8c3373efc6a248f391552076ee26739d",
            "f123aae02dc74afebed3d60cced666a6",
            "8a3a8f3f2e704ed1bbb98f51bfe439b6",
            "6abfbf50785040188e8c639ff6a79b47",
            "ebe00e6844ad453c9508efa37ff9b9f1",
            "82fdac1ca8aa47168969417580c66c97",
            "2e12b025d32841038b8669e02dd12ab9",
            "eb05c43578cc4c25bc30747834189fb7",
            "efc3d88afbaf4f049a142ee3eab1fde6",
            "8e911bbec61c4ac0aef72a2cc838eab4",
            "938e373ba29a421fa270f60fc608eb74",
            "f0306e71e6034ebd80f4b4b0082dc61d",
            "7281080caf7c46ca9def096e45e40fba",
            "db93f31fab684a1693264a569abe2a48",
            "85dba72134d5449eb43653fd5b0196bc",
            "d1b40a6579ee4bbdbe1025412c416a91",
            "fcc4dc00b33f47769f01b92c41f93d16",
            "1c452424c1864ea399a90f4bfa5492a3",
            "3d168f9a38ee4d9fa53e221f97ee2b7a",
            "e73afa9a3f7d4c7eb259b0d712985cf5",
            "0fe9933f34234ad2b9824841f5d0a930",
            "bfe15b2e962b48389e6e002532b5027b",
            "9432dcf011eb48d9bca934250bbe20b5",
            "0fe7d90d5c264bdda94e6b0ac7684b7b",
            "44653e62764d4b47b7e0f415017224c2",
            "bb0a88e67f034a41b380399241c7c5d2",
            "bc80a1d97d8d41719fae1820e38cd7ed",
            "767bdbd35a81426084fd71f2ca4ebb38",
            "fa17bebc02bc423f84c35ccdbc55eef9",
            "e4a97bc53bac4dd99b006f53a0024d37",
            "6c5c126298164050b0bef0a85619520c",
            "e3002ab53e844b68a726521de058e060",
            "477a1626c03a43aeae310795def0144d",
            "904e271d50ff4bc8befb6f511ea17952",
            "40245dcc8c474a4b8b8653ce89fab06c",
            "2f141255863946eca9ffe2d0485d408a"
          ]
        },
        "id": "aBabd43BgFsx",
        "outputId": "113700f6-58ee-4dbc-eec0-4adc4a51c197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "m3hrdadfi/wav2vec2-large-xlsr-persian-v2 cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/158 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e24a736eb8f343868a54afe61df3e0a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a93dc4e86f84436eb8fb9fd56beec584"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c0f7896c26a466a886c11539af885d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:370: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/398 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6abfbf50785040188e8c639ff6a79b47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85dba72134d5449eb43653fd5b0196bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb0a88e67f034a41b380399241c7c5d2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name_or_path = \"m3hrdadfi/wav2vec2-large-xlsr-persian-v2\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(model_name_or_path, device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name_or_path).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egbF_GZz6LZL"
      },
      "outputs": [],
      "source": [
        "vocab_dict = processor.tokenizer.get_vocab()\n",
        "sort_vocab = sorted((value, key) for (key,value) in vocab_dict.items())\n",
        "\n",
        "vocab = []\n",
        "for _, token in sort_vocab:\n",
        "    vocab.append(token.lower())\n",
        "\n",
        "vocab[vocab.index(processor.tokenizer.word_delimiter_token)] = ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I092xI_zlrSP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xriFGEWQkO4M"
      },
      "outputs": [],
      "source": [
        "tokenizer = Wav2Vec2CTCTokenizer(\n",
        "    \"/content/vocab.json\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    word_delimiter_token=\"|\",\n",
        "    do_lower_case=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6qREbLF1wqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e9c162-60f6-4065-b05b-ff645739f3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading words_correct.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 1461977088 bytes == 0x56113804e000 @  0x7f47bfe0c1e7 0x561132656b8e 0x56113264b890 0x561132628adb 0x56113262b6a4 0x5611326200c5 0x7f47be650c87 0x56113262083a\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ],
      "source": [
        "!/content/kenlm/build/bin/build_binary -T -s trie words_correct.arpa lm4.binary\n",
        "!rm words_correct.arpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1JbnvRHcGj2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b5fbbc-8184-46b8-c7c1-9d3587854391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyctcdecode.decoder:Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.\n",
            "WARNING:pyctcdecode.alphabet:Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
            "WARNING:pyctcdecode.language_model:No known unigrams provided, decoding results might be a lot worse.\n"
          ]
        }
      ],
      "source": [
        "from pyctcdecode import build_ctcdecoder\n",
        "\n",
        "decoder = build_ctcdecoder(\n",
        "    labels=vocab,\n",
        "    kenlm_model_path=\"lm4.binary\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QVbFSaraZXC"
      },
      "outputs": [],
      "source": [
        "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    tokenizer=tokenizer,\n",
        "    decoder=decoder\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JA-_cN9J-gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8efe8bd-fe95-4719-c827-567c5a3ec65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 13.97 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14102, done.        \n",
            "remote: Counting objects: 100% (415/415), done.        \n",
            "remote: Compressing objects: 100% (289/289), done.        \n",
            "remote: Total 14102 (delta 127), reused 382 (delta 112), pack-reused 13687        \n",
            "Receiving objects: 100% (14102/14102), 5.89 MiB | 17.64 MiB/s, done.\n",
            "Resolving deltas: 100% (8007/8007), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13405142 sha256=4b6403c10984cc62a9c3ce6ce844cc23db0f20d19fa540c44b1be5bdae17a467\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bxymebj3/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "Successfully installed ctcdecode-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!cd ctcdecode && pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r8cplvxKCJO"
      },
      "outputs": [],
      "source": [
        "beam_decoder = BeamCTCDecoder(vocab, lm_path='lm4.binary',\n",
        "                                # alpha=0.6, beta=0.8,\n",
        "                                alpha=1.0, beta=3.5,        # fine-tuned\n",
        "                                cutoff_top_n=80, cutoff_prob=1.0,\n",
        "                                beam_width=400, num_processes=16,\n",
        "                                blank_index=vocab.index(processor.tokenizer.pad_token))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFhpKf5fK4bq"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/aliyzd95/ShEMO-Modification/raw/main/shemo.zip\n",
        "!unzip shemo.zip\n",
        "!rm shemo.zip\n",
        "\n",
        "!wget https://github.com/aliyzd95/ShEMO-Modification/raw/main/modified_shemo.json\n",
        "\n",
        "!wget https://github.com/pariajm/sharif-emotional-speech-dataset/raw/master/shemo.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "archl-9_LMZg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('shemo.json', encoding='utf-8') as os:\n",
        "    original_shemo = json.loads(os.read())\n",
        "\n",
        "with open('modified_shemo.json', encoding='utf-8') as ms:\n",
        "    modified_shemo = json.loads(ms.read())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_dataset(MODE):\n",
        "\n",
        "    shemo = []\n",
        "\n",
        "    if MODE=='modified':\n",
        "        for name in modified_shemo:\n",
        "            sample = {}\n",
        "\n",
        "            modified = modified_shemo[name]\n",
        "            sample[\"sentence\"] = normalizer(modified[\"transcript\"])\n",
        "            sample[\"path\"] = modified[\"path\"]\n",
        "\n",
        "            shemo.append(sample)\n",
        "\n",
        "    elif MODE=='original':\n",
        "\n",
        "        import os\n",
        "        for file in os.listdir('shemo'):\n",
        "            sample = {}\n",
        "            file_name = file[:6]\n",
        "\n",
        "            try:\n",
        "                original = original_shemo[file_name]\n",
        "                sample[\"sentence\"] = normalizer(original[\"transcript\"])\n",
        "                sample[\"path\"] = f'shemo/{file}'\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            shemo.append(sample)\n",
        "\n",
        "    else:\n",
        "        print('MODE must be one of \"modified\" or \"original\"')\n",
        "\n",
        "    return shemo"
      ],
      "metadata": {
        "id": "IannN5NynMCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODE = 'modified'   ### 'original' or 'modified'\n",
        "shemo = select_dataset(MODE)\n",
        "print(f'the {MODE}_ShEMO has {len(shemo)} samples.')"
      ],
      "metadata": {
        "id": "I_JvKy7-r8fY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcde6fc-e7f6-48db-b8f7-a99ee30afdaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the modified_ShEMO has 3000 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUvstblHJ8KR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ab06da1-66f5-46ca-eeb0-13103e6f0eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/datasets.git\n",
            "  Cloning https://github.com/huggingface/datasets.git to /tmp/pip-req-build-ki3hnggh\n",
            "  Running command git clone -q https://github.com/huggingface/datasets.git /tmp/pip-req-build-ki3hnggh\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (3.8.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (2022.10.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (6.0.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 61.7 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (21.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (1.3.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets==2.7.1.dev0) (0.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (4.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (1.8.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.7.1.dev0) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.7.1.dev0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==2.7.1.dev0) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.7.1.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.7.1.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.7.1.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.7.1.dev0) (2022.9.24)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==2.7.1.dev0) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==2.7.1.dev0) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==2.7.1.dev0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==2.7.1.dev0) (1.15.0)\n",
            "Building wheels for collected packages: datasets\n",
            "  Building wheel for datasets (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datasets: filename=datasets-2.7.1.dev0-py3-none-any.whl size=448443 sha256=2239cc3b69f8b82b9b4356d437577dbf784bbbd26891c179c97e8a0aa4158ad7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b2259ey_/wheels/cc/33/c1/2cacc415b23189a83908e45db67381ba26175ef1e8aa9062aa\n",
            "Successfully built datasets\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.7.1.dev0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.12.1+cu113)\n",
            "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchaudio) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.4.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.6)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (4.13.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
            "Collecting levenshtein==0.20.2\n",
            "  Downloading Levenshtein-0.20.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 15.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from levenshtein==0.20.2->jiwer) (2.13.2)\n",
            "Installing collected packages: levenshtein, jiwer\n",
            "  Attempting uninstall: levenshtein\n",
            "    Found existing installation: Levenshtein 0.20.8\n",
            "    Uninstalling Levenshtein-0.20.8:\n",
            "      Successfully uninstalled Levenshtein-0.20.8\n",
            "Successfully installed jiwer-2.5.1 levenshtein-0.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "Levenshtein"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_elfP8zbJ-JF"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_metric, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPKtl9elKHm0"
      },
      "outputs": [],
      "source": [
        "def speech_file_to_array_fn(batch):\n",
        "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
        "    speech_array = speech_array.squeeze().numpy()\n",
        "    speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor_with_lm.feature_extractor.sampling_rate)\n",
        "\n",
        "    batch[\"speech\"] = speech_array\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9fKhYPZKePN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5cd99bc3fd864cee9b66aad86549e423",
            "08fb04e7df8c4921b5404ce2011cdd4d",
            "d593961c8fde4468b480ecfb554425c2",
            "a6c44fae76e94d989b56a0108803577a",
            "8015fe0604184414a22cafa68f1e8dc4",
            "4da65e3195fe49fd8147decde1894541",
            "87a085fbbd2f40b3a0d4ef29a1a748a0",
            "86c328aedf06442a83bf6cb614e96916",
            "c97bf3883a524a76a0cd628ea1f5c749",
            "a863019d0f7b4b159e03779f4b6f3148",
            "6bd294a5f3bf4dffbb5a461d99d2bd52"
          ]
        },
        "outputId": "6aa98845-7073-4974-e7e5-c8261c7cc0d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3000 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cd99bc3fd864cee9b66aad86549e423"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = Dataset.from_pandas(pd.DataFrame(data=shemo))\n",
        "dataset = dataset.map(speech_file_to_array_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcDdHo66gQlR"
      },
      "outputs": [],
      "source": [
        "def predict(batch):\n",
        "\n",
        "    features = processor_with_lm(\n",
        "        batch[\"speech\"],\n",
        "        sampling_rate=processor_with_lm.feature_extractor.sampling_rate,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    input_values = features.input_values.to(device)\n",
        "    attention_mask = features.attention_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values, attention_mask=attention_mask).logits\n",
        "\n",
        "    beam_decoded_output, beam_decoded_offsets = beam_decoder.decode(logits)\n",
        "\n",
        "    batch[\"predicted\"] = beam_decoded_output[0][0]\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = dataset.map(predict, batched=False)"
      ],
      "metadata": {
        "id": "sLOCJoLDmOTQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "7f497eec3f064cc688f213c6b6ae40b9",
            "61b37555059f43deb0e0d876314a0359",
            "cb2e702913674f108e64aaf97844a6df",
            "11abeebcadd04db49d07937c6361d66c",
            "7cce4dc7cf4448e497d3a750d5d39c1d",
            "eac48f5f3dad429cafcfbe1a997ace8a",
            "30c4acd8eb0e4b1d885ed45b83a59312",
            "e68f480d1ab1405389e2d4ad4b185139",
            "f72d0c063da34e40982366bec7f6a0c2",
            "3f4795d2585b4cd6a77d59f8fb6192ac",
            "9f2f634eac41401cb5de83d97418846d"
          ]
        },
        "outputId": "68fb08fd-a3bc-48d5-d1ff-03461cb5f27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dill/_dill.py:1890: PicklingWarning: Pickling a PyCapsule (None) does not pickle any C data structures and could cause segmentation faults or other memory errors when unpickling.\n",
            "  warnings.warn('Pickling a PyCapsule (%s) does not pickle any C data structures and could cause segmentation faults or other memory errors when unpickling.' % (name,), PicklingWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3000 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f497eec3f064cc688f213c6b6ae40b9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFxI0aqUKop1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "8202a9ed63f6468cb6720b2a4aa9a5f9",
            "0fce19a069b146ccbc6efd9bc1667ed4",
            "cc3b58c7ea0f4c019fbbc6623d6e7d4c",
            "de8cc79aa0a048df85a4da457a3b1ff2",
            "b32f8edacf664cf1b878fffc5a4f4ebe",
            "c796cbe1426848a59f7ef34dcc21e6fe",
            "47d8703b6195423b8081ac0c0f8e9861",
            "24638c49bbb643198b079b93b69b243b",
            "d00f385cc4e74e09b23ee975ac3e1380",
            "696e906594e64c0a8c18bc76c7ae877d",
            "b6772c1081a54607bc2e829e82a1f197"
          ]
        },
        "outputId": "30af320f-0559-45ee-ebe5-47172674e795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8202a9ed63f6468cb6720b2a4aa9a5f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER on modified_ShEMO: 30.796283813487296\n"
          ]
        }
      ],
      "source": [
        "wer = load_metric(\"wer\")\n",
        "print(f\"WER on {MODE}_ShEMO: {100 * wer.compute(predictions=result['predicted'], references=result['sentence'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import *\n",
        "\n",
        "count = 0\n",
        "for data in result:\n",
        "\n",
        "    target = data[\"sentence\"]\n",
        "    predict = data[\"predicted\"]\n",
        "\n",
        "    w = wer(target, predict)\n",
        "    c = cer(target, predict)\n",
        "\n",
        "    if w>0.5 and c>0.5:\n",
        "        count += 1\n",
        "        print(data[\"path\"])\n",
        "        print(f'sentence: {target}, predicted: {predict}')\n",
        "        print(f'wer={w}, cer={c}')\n",
        "        print('==================================================')\n",
        "\n",
        "print(f'There are {count} files out of a total of {len(result)} files in the dataset that has wer>0.5 and cer>0.5')"
      ],
      "metadata": {
        "id": "5Tp2s57l3nrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ccf613-d15a-449a-8f53-1dbc8ca800de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shemo/F01A19.wav\n",
            "sentence: تو که لب به هیچی نزدی , predicted: که نبی شینا زدیم \n",
            "wer=0.8333333333333334, cer=0.5714285714285714\n",
            "==================================================\n",
            "shemo/F01H03.wav\n",
            "sentence: نگفتم , predicted: نگفتم کن \n",
            "wer=1.0, cer=0.6\n",
            "==================================================\n",
            "shemo/F01S04.wav\n",
            "sentence: دیگه از دستش خسته شدم , predicted: یک حس در بهشت خسته شده \n",
            "wer=1.0, cer=0.5238095238095238\n",
            "==================================================\n",
            "shemo/F01S09.wav\n",
            "sentence: انگار اصلا قلب تو سینه اش نیست , predicted: این کار این قید دوستی ناشی \n",
            "wer=1.0, cer=0.6\n",
            "==================================================\n",
            "shemo/F02F01.wav\n",
            "sentence: نه آقا نه , predicted: نمونه \n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/F03F01.wav\n",
            "sentence: من قربان , predicted: اما من همونو بقای بن نگه \n",
            "wer=2.5, cer=2.25\n",
            "==================================================\n",
            "shemo/F07S01.wav\n",
            "sentence: بنجی بیچاره من , predicted: و این چیه بیچ هیات به \n",
            "wer=2.0, cer=1.0\n",
            "==================================================\n",
            "shemo/F07S20.wav\n",
            "sentence: جورج نه با بی رحمی ازش گرفتم , predicted: چه ارزونه شی با بی روی از هجرت \n",
            "wer=0.8571428571428571, cer=0.5714285714285714\n",
            "==================================================\n",
            "shemo/F07W01.wav\n",
            "sentence: ده دلار , predicted: دونا \n",
            "wer=1.0, cer=0.7142857142857143\n",
            "==================================================\n",
            "shemo/F16F01.wav\n",
            "sentence: یه نفر داره زنگ می زنه , predicted: یه پارت رینگ بیست الی \n",
            "wer=0.8333333333333334, cer=0.6363636363636364\n",
            "==================================================\n",
            "shemo/F16F02.wav\n",
            "sentence: این این دیوونگی محضه , predicted: بدون دیوونگی ما هستی \n",
            "wer=1.0, cer=0.55\n",
            "==================================================\n",
            "shemo/F21W13.wav\n",
            "sentence: دورانت , predicted: در \n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/F22H02.wav\n",
            "sentence: موافقم , predicted: او با تریخا\n",
            "wer=3.0, cer=1.5\n",
            "==================================================\n",
            "shemo/F22S08.wav\n",
            "sentence: چه جوری می تونم برگردم سمتش درصورتی که اون اون بی عاطفه , predicted: تونی می تونم برگردم سمت دیگه اون اون بیاندازید و با بی کنم \n",
            "wer=0.75, cer=0.5454545454545454\n",
            "==================================================\n",
            "shemo/F24H04.wav\n",
            "sentence: آره شادی آره , predicted: ارشاد یوری \n",
            "wer=1.0, cer=0.5833333333333334\n",
            "==================================================\n",
            "shemo/F24H18.wav\n",
            "sentence: منبعش موثقه , predicted: راه منبعش موسقی \n",
            "wer=1.0, cer=0.5454545454545454\n",
            "==================================================\n",
            "shemo/F24W20.wav\n",
            "sentence: بله , predicted: ولی \n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/F24W24.wav\n",
            "sentence: کی , predicted: شیی\n",
            "wer=1.0, cer=1.0\n",
            "==================================================\n",
            "shemo/F31S01.wav\n",
            "sentence: که این طور , predicted: جدید و \n",
            "wer=1.0, cer=0.7\n",
            "==================================================\n",
            "shemo/M03A06.wav\n",
            "sentence: گمشو اونور , predicted: گم شدن است \n",
            "wer=1.5, cer=0.7\n",
            "==================================================\n",
            "shemo/M03A10.wav\n",
            "sentence: گمشو , predicted: گم چهار \n",
            "wer=2.0, cer=1.25\n",
            "==================================================\n",
            "shemo/M03A14.wav\n",
            "sentence: گوش نکن , predicted: کوشکو\n",
            "wer=1.0, cer=0.5714285714285714\n",
            "==================================================\n",
            "shemo/M11H08.wav\n",
            "sentence: بگو عزیزم , predicted: نخبه و عزیزه \n",
            "wer=1.5, cer=0.5555555555555556\n",
            "==================================================\n",
            "shemo/M11H09.wav\n",
            "sentence: ای شیطون , predicted: ای شهید تلفن \n",
            "wer=1.0, cer=0.75\n",
            "==================================================\n",
            "shemo/M12A65.wav\n",
            "sentence: به سلامت , predicted: و چلوم\n",
            "wer=1.0, cer=0.625\n",
            "==================================================\n",
            "shemo/M16A08.wav\n",
            "sentence: بزنم دنده هاتو داغون کنم زن , predicted: تا پسر تندتر داغ کرد شد \n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/M16A14.wav\n",
            "sentence: بده شوهرت دکون دار بشه , predicted: من است محل دو کون دار بشه \n",
            "wer=1.0, cer=0.5454545454545454\n",
            "==================================================\n",
            "shemo/M16A19.wav\n",
            "sentence: د میاری یا نه , predicted: می یاریار\n",
            "wer=1.0, cer=0.6153846153846154\n",
            "==================================================\n",
            "shemo/M16A23.wav\n",
            "sentence: د حرف بزن د , predicted: و به سن ده \n",
            "wer=1.0, cer=0.6363636363636364\n",
            "==================================================\n",
            "shemo/M20W05.wav\n",
            "sentence: آخه مزاحم چی , predicted: وقتی ما همه چی \n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/M21W04.wav\n",
            "sentence: بله , predicted: بلین\n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/M22W01.wav\n",
            "sentence: عبدرضا , predicted: ب درس \n",
            "wer=2.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/M24W01.wav\n",
            "sentence: جدی , predicted: تی \n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/M24W04.wav\n",
            "sentence: جدی می گی , predicted: چه تو میگی \n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/M26A29.wav\n",
            "sentence: یا این یا او , predicted: یاییلی\n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/M26W03.wav\n",
            "sentence: عجب , predicted: جپ\n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "shemo/M28W03.wav\n",
            "sentence: چرا آخه , predicted: چه راخه\n",
            "wer=1.0, cer=0.5714285714285714\n",
            "==================================================\n",
            "shemo/M41H05.wav\n",
            "sentence: تو رادیو , predicted: مهر تو رادیه\n",
            "wer=1.0, cer=0.625\n",
            "==================================================\n",
            "shemo/M50A11.wav\n",
            "sentence: حرف بزن , predicted: هر گزان\n",
            "wer=1.0, cer=0.5714285714285714\n",
            "==================================================\n",
            "shemo/M53A07.wav\n",
            "sentence: پس بگو , predicted: سمکو \n",
            "wer=1.0, cer=0.6666666666666666\n",
            "==================================================\n",
            "There are 40 files out of a total of 3000 files in the dataset that has wer>0.5 and cer>0.5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}